{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05eda315",
   "metadata": {},
   "source": [
    "#  Notebook 04: Anomaly Detection & Fraud Analysis\n",
    "\n",
    "## AADHAAR INTELLIGENCE SYSTEM - LENS 3\n",
    "\n",
    "---\n",
    "\n",
    "### Objective\n",
    "Detect fraudulent patterns and anomalies in **REAL UIDAI Aadhaar Data** using:\n",
    "- Enrollment pattern anomaly analysis\n",
    "- Isolation Forest for anomaly detection\n",
    "- DBSCAN clustering for fraud ring identification\n",
    "\n",
    "### Data Sources (Real UIDAI Data)\n",
    "- **Enrolment Data**: Age-wise enrollment by pincode/district\n",
    "- **Demographic Data**: Demographic update patterns\n",
    "- **Biometric Data**: Biometric update patterns\n",
    "\n",
    "### Methods\n",
    "- **Isolation Forest**: Unsupervised anomaly detection (2% contamination)\n",
    "- **DBSCAN**: Density-based clustering for fraud rings\n",
    "- **Risk Scoring**: Multi-factor risk assessment\n",
    "\n",
    "### Key Focus Areas\n",
    "- Unusual enrollment spikes at specific pincodes\n",
    "- Abnormal age distribution patterns\n",
    "- Geographic anomaly clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7602354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 1: Import Libraries\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\" Libraries imported successfully\")\n",
    "print(f\" Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 2: Load Real UIDAI Datasets\n",
    "# ============================================\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = '../outputs/'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{OUTPUT_DIR}/charts\", exist_ok=True)\n",
    "\n",
    "# Data paths\n",
    "DATA_DIR = '../data/'\n",
    "ENROL_DIR = f\"{DATA_DIR}enrolment/\"\n",
    "DEMO_DIR = f\"{DATA_DIR}demographic/\"\n",
    "BIO_DIR = f\"{DATA_DIR}biometric/\"\n",
    "\n",
    "print(\" LOADING REAL UIDAI DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load Enrolment Data\n",
    "print(\"\\n1️⃣ Loading Enrolment Data...\")\n",
    "enrol_files = glob.glob(f\"{ENROL_DIR}*.csv\")\n",
    "df_list = []\n",
    "for f in enrol_files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_list.append(df_temp)\n",
    "df_enrolment = pd.concat(df_list, ignore_index=True)\n",
    "print(f\"    Loaded {len(df_enrolment):,} records from {len(enrol_files)} files\")\n",
    "\n",
    "# Load Demographic Data\n",
    "print(\"\\n2️⃣ Loading Demographic Data...\")\n",
    "demo_files = glob.glob(f\"{DEMO_DIR}*.csv\")\n",
    "df_list = []\n",
    "for f in demo_files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_list.append(df_temp)\n",
    "df_demographic = pd.concat(df_list, ignore_index=True)\n",
    "print(f\"    Loaded {len(df_demographic):,} records from {len(demo_files)} files\")\n",
    "\n",
    "# Load Biometric Data\n",
    "print(\"\\n3️⃣ Loading Biometric Data...\")\n",
    "bio_files = glob.glob(f\"{BIO_DIR}*.csv\")\n",
    "df_list = []\n",
    "for f in bio_files:\n",
    "    df_temp = pd.read_csv(f)\n",
    "    df_list.append(df_temp)\n",
    "df_biometric = pd.concat(df_list, ignore_index=True)\n",
    "print(f\"    Loaded {len(df_biometric):,} records from {len(bio_files)} files\")\n",
    "\n",
    "# Parse dates\n",
    "df_enrolment['date'] = pd.to_datetime(df_enrolment['date'], format='%d-%m-%Y', errors='coerce')\n",
    "df_demographic['date'] = pd.to_datetime(df_demographic['date'], format='%d-%m-%Y', errors='coerce')\n",
    "df_biometric['date'] = pd.to_datetime(df_biometric['date'], format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "# Create total enrollment column\n",
    "df_enrolment['total_enrolments'] = df_enrolment['age_0_5'] + df_enrolment['age_5_17'] + df_enrolment['age_18_greater']\n",
    "\n",
    "# Create totals for demographic and biometric\n",
    "demo_cols = [c for c in df_demographic.columns if 'demo_age' in c]\n",
    "df_demographic['total_demo'] = df_demographic[demo_cols].sum(axis=1)\n",
    "\n",
    "bio_cols = [c for c in df_biometric.columns if 'bio_age' in c]\n",
    "df_biometric['total_bio'] = df_biometric[bio_cols].sum(axis=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" DATASETS LOADED SUCCESSFULLY!\")\n",
    "print(f\"   Total Enrolment Records: {len(df_enrolment):,}\")\n",
    "print(f\"   Total Demographic Records: {len(df_demographic):,}\")\n",
    "print(f\"   Total Biometric Records: {len(df_biometric):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ced750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 3: Feature Engineering for Anomaly Detection\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n FEATURE ENGINEERING FOR FRAUD DETECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create pincode-level aggregations from enrollment data\n",
    "pincode_stats = df_enrolment.groupby('pincode').agg({\n",
    "    'total_enrolments': ['sum', 'mean', 'std', 'count'],\n",
    "    'age_0_5': 'sum',\n",
    "    'age_5_17': 'sum',\n",
    "    'age_18_greater': 'sum',\n",
    "    'state': 'first',\n",
    "    'district': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "pincode_stats.columns = ['pincode', 'total_enrolments', 'avg_daily_enrol', 'std_enrol', \n",
    "                         'num_days', 'total_0_5', 'total_5_17', 'total_18_plus',\n",
    "                         'state', 'district']\n",
    "\n",
    "# Calculate derived metrics\n",
    "pincode_stats['std_enrol'] = pincode_stats['std_enrol'].fillna(0)\n",
    "\n",
    "# Age distribution percentages\n",
    "pincode_stats['pct_0_5'] = (pincode_stats['total_0_5'] / pincode_stats['total_enrolments'] * 100).fillna(0)\n",
    "pincode_stats['pct_5_17'] = (pincode_stats['total_5_17'] / pincode_stats['total_enrolments'] * 100).fillna(0)\n",
    "pincode_stats['pct_18_plus'] = (pincode_stats['total_18_plus'] / pincode_stats['total_enrolments'] * 100).fillna(0)\n",
    "\n",
    "# Coefficient of variation (volatility indicator)\n",
    "pincode_stats['cv_enrol'] = (pincode_stats['std_enrol'] / pincode_stats['avg_daily_enrol'] * 100).fillna(0)\n",
    "pincode_stats['cv_enrol'] = pincode_stats['cv_enrol'].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# Merge with demographic data\n",
    "demo_by_pin = df_demographic.groupby('pincode')['total_demo'].sum().reset_index()\n",
    "pincode_stats = pincode_stats.merge(demo_by_pin, on='pincode', how='left')\n",
    "pincode_stats['total_demo'] = pincode_stats['total_demo'].fillna(0)\n",
    "\n",
    "# Merge with biometric data  \n",
    "bio_by_pin = df_biometric.groupby('pincode')['total_bio'].sum().reset_index()\n",
    "pincode_stats = pincode_stats.merge(bio_by_pin, on='pincode', how='left')\n",
    "pincode_stats['total_bio'] = pincode_stats['total_bio'].fillna(0)\n",
    "\n",
    "# Calculate update ratios (demo + bio vs enrollments)\n",
    "pincode_stats['update_ratio'] = ((pincode_stats['total_demo'] + pincode_stats['total_bio']) / \n",
    "                                  pincode_stats['total_enrolments'] * 100).fillna(0)\n",
    "pincode_stats['update_ratio'] = pincode_stats['update_ratio'].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "print(f\" Created {len(pincode_stats):,} pincode-level aggregations\")\n",
    "print(f\"\\n Feature Statistics:\")\n",
    "print(pincode_stats[['total_enrolments', 'avg_daily_enrol', 'cv_enrol', 'pct_0_5', 'pct_18_plus', 'update_ratio']].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f618f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 4: Isolation Forest Anomaly Detection\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n ISOLATION FOREST ANOMALY DETECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Select features for anomaly detection\n",
    "anomaly_features = ['avg_daily_enrol', 'std_enrol', 'cv_enrol', \n",
    "                    'pct_0_5', 'pct_5_17', 'pct_18_plus', \n",
    "                    'update_ratio']\n",
    "\n",
    "X = pincode_stats[anomaly_features].copy()\n",
    "\n",
    "# Handle missing/infinite values\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Fit Isolation Forest\n",
    "CONTAMINATION = 0.02  # Expect 2% anomalies\n",
    "\n",
    "iso_forest = IsolationForest(\n",
    "    n_estimators=200,\n",
    "    contamination=CONTAMINATION,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Predict anomalies (-1 = anomaly, 1 = normal)\n",
    "pincode_stats['anomaly_label'] = iso_forest.fit_predict(X_scaled)\n",
    "pincode_stats['anomaly_score'] = -iso_forest.score_samples(X_scaled)  # Higher = more anomalous\n",
    "\n",
    "# Normalize anomaly score to 0-100 risk score\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 100))\n",
    "pincode_stats['risk_score'] = min_max_scaler.fit_transform(\n",
    "    pincode_stats[['anomaly_score']]\n",
    ").flatten()\n",
    "\n",
    "# Count anomalies\n",
    "n_anomalies = (pincode_stats['anomaly_label'] == -1).sum()\n",
    "anomaly_pct = n_anomalies / len(pincode_stats) * 100\n",
    "\n",
    "print(f\"\\n ANOMALY DETECTION RESULTS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"   Total Pincodes Analyzed: {len(pincode_stats):,}\")\n",
    "print(f\"   Anomalies Detected: {n_anomalies:,} ({anomaly_pct:.1f}%)\")\n",
    "print(f\"   Contamination Parameter: {CONTAMINATION*100:.0f}%\")\n",
    "print(f\"   Average Risk Score: {pincode_stats['risk_score'].mean():.1f}\")\n",
    "print(f\"   Max Risk Score: {pincode_stats['risk_score'].max():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba51f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 5: Top Anomalous Pincodes\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n TOP HIGH-RISK PINCODES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get anomalous pincodes\n",
    "anomalies = pincode_stats[pincode_stats['anomaly_label'] == -1].copy()\n",
    "anomalies = anomalies.sort_values('risk_score', ascending=False)\n",
    "\n",
    "print(\"\\n TOP 20 SUSPICIOUS PINCODES:\")\n",
    "print(\"-\" * 120)\n",
    "print(f\"{'Rank':<5} {'Pincode':<10} {'State':<25} {'District':<20} {'Total Enrol':>12} {'CV%':>8} {'Risk':>8}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for i, (_, row) in enumerate(anomalies.head(20).iterrows(), 1):\n",
    "    state_name = str(row['state'])[:24] if pd.notna(row['state']) else 'N/A'\n",
    "    district_name = str(row['district'])[:19] if pd.notna(row['district']) else 'N/A'\n",
    "    print(f\"{i:<5} {row['pincode']:<10} {state_name:<25} {district_name:<20} {row['total_enrolments']:>12,.0f} {row['cv_enrol']:>8.1f} {row['risk_score']:>8.1f}\")\n",
    "\n",
    "# Calculate estimated suspicious volume\n",
    "total_suspicious_enrolments = anomalies['total_enrolments'].sum()\n",
    "\n",
    "print(f\"\\n SUSPICIOUS ACTIVITY SUMMARY:\")\n",
    "print(f\"   Total Suspicious Pincodes: {len(anomalies):,}\")\n",
    "print(f\"   Total Enrolments at Risk: {total_suspicious_enrolments:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adad561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 6: DBSCAN Fraud Ring Detection\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n FRAUD RING DETECTION (DBSCAN CLUSTERING)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Focus on anomalous pincodes for fraud ring detection\n",
    "high_risk = pincode_stats[pincode_stats['risk_score'] > 70].copy()\n",
    "\n",
    "if len(high_risk) > 10:\n",
    "    # Features for clustering\n",
    "    cluster_features = ['cv_enrol', 'pct_0_5', 'pct_18_plus', 'update_ratio']\n",
    "    X_cluster = high_risk[cluster_features].values\n",
    "    \n",
    "    # Handle infinities\n",
    "    X_cluster = np.nan_to_num(X_cluster, nan=0, posinf=0, neginf=0)\n",
    "    \n",
    "    # Scale\n",
    "    X_cluster_scaled = StandardScaler().fit_transform(X_cluster)\n",
    "    \n",
    "    # DBSCAN clustering\n",
    "    dbscan = DBSCAN(eps=0.8, min_samples=3)\n",
    "    high_risk['fraud_cluster'] = dbscan.fit_predict(X_cluster_scaled)\n",
    "    \n",
    "    # Analyze clusters\n",
    "    cluster_counts = high_risk['fraud_cluster'].value_counts()\n",
    "    n_clusters = len(cluster_counts[cluster_counts.index != -1])\n",
    "    \n",
    "    print(f\"\\n CLUSTERING RESULTS:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"   High-Risk Pincodes Analyzed: {len(high_risk):,}\")\n",
    "    print(f\"   Fraud Rings Detected: {n_clusters}\")\n",
    "    print(f\"   Noise Points (isolated anomalies): {(high_risk['fraud_cluster'] == -1).sum()}\")\n",
    "    \n",
    "    # Identify largest fraud ring\n",
    "    if n_clusters > 0:\n",
    "        largest_cluster = cluster_counts[cluster_counts.index != -1].idxmax()\n",
    "        largest_cluster_size = cluster_counts[largest_cluster]\n",
    "        \n",
    "        fraud_ring = high_risk[high_risk['fraud_cluster'] == largest_cluster]\n",
    "        fraud_ring_states = fraud_ring['state'].value_counts()\n",
    "        \n",
    "        print(f\"\\n LARGEST FRAUD RING (Cluster {largest_cluster}):\")\n",
    "        print(f\"   Pincodes Involved: {largest_cluster_size}\")\n",
    "        print(f\"   Primary State: {fraud_ring_states.index[0] if len(fraud_ring_states) > 0 else 'Unknown'}\")\n",
    "        print(f\"   Avg CV%: {fraud_ring['cv_enrol'].mean():.1f}%\")\n",
    "        print(f\"   Total Enrolments: {fraud_ring['total_enrolments'].sum():,.0f}\")\n",
    "else:\n",
    "    high_risk['fraud_cluster'] = -1\n",
    "    n_clusters = 0\n",
    "    print(\"⚠️ Not enough high-risk pincodes for cluster analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe6923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 7: Risk Heatmap Visualization\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n CREATING RISK HEATMAP VISUALIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Aggregate by state\n",
    "state_risk_agg = pincode_stats.groupby('state').agg({\n",
    "    'pincode': 'count',\n",
    "    'risk_score': 'mean',\n",
    "    'total_enrolments': 'sum',\n",
    "    'cv_enrol': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "state_risk_agg.columns = ['State', 'Pincodes', 'Avg Risk Score', 'Total Enrollments', 'Avg CV%']\n",
    "state_risk_agg = state_risk_agg.sort_values('Avg Risk Score', ascending=False)\n",
    "\n",
    "# Top 20 states\n",
    "state_risk_top = state_risk_agg.head(20)\n",
    "\n",
    "# Create risk chart\n",
    "fig_risk = go.Figure()\n",
    "\n",
    "fig_risk.add_trace(go.Bar(\n",
    "    x=state_risk_top['Avg Risk Score'],\n",
    "    y=state_risk_top['State'],\n",
    "    orientation='h',\n",
    "    marker_color=['#D62828' if r > 50 else '#F77F00' if r > 30 else '#1B998B' \n",
    "                  for r in state_risk_top['Avg Risk Score']],\n",
    "    text=[f\"{r:.1f}\" for r in state_risk_top['Avg Risk Score']],\n",
    "    textposition='outside',\n",
    "    name='Risk Score'\n",
    "))\n",
    "\n",
    "fig_risk.update_layout(\n",
    "    title=dict(\n",
    "        text='<b>FRAUD RISK ANALYSIS BY STATE</b><br><sup> Red = High Risk |  Orange = Medium |  Green = Low</sup>',\n",
    "        x=0.5\n",
    "    ),\n",
    "    xaxis_title='Average Risk Score',\n",
    "    yaxis_title='State',\n",
    "    height=600,\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig_risk.write_html(f\"{OUTPUT_DIR}/charts/04_risk_by_state.html\")\n",
    "print(\" Risk by state chart saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4522a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 8: Age Distribution Anomalies\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n AGE DISTRIBUTION ANOMALY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find pincodes with unusual age distributions\n",
    "# Normal expectation: 0-5 ~10%, 5-17 ~20%, 18+ ~70%\n",
    "\n",
    "# Flag unusual patterns\n",
    "pincode_stats['unusual_child_ratio'] = (pincode_stats['pct_0_5'] > 30) | (pincode_stats['pct_0_5'] < 1)\n",
    "pincode_stats['unusual_adult_ratio'] = (pincode_stats['pct_18_plus'] > 95) | (pincode_stats['pct_18_plus'] < 40)\n",
    "\n",
    "unusual_age = pincode_stats[(pincode_stats['unusual_child_ratio']) | (pincode_stats['unusual_adult_ratio'])]\n",
    "\n",
    "print(f\"\\n AGE DISTRIBUTION ANOMALIES:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"   Unusual Child Ratio (0-5): {pincode_stats['unusual_child_ratio'].sum():,} pincodes\")\n",
    "print(f\"   Unusual Adult Ratio (18+): {pincode_stats['unusual_adult_ratio'].sum():,} pincodes\")\n",
    "print(f\"   Total Flagged: {len(unusual_age):,} pincodes\")\n",
    "\n",
    "# Top unusual by child ratio\n",
    "print(\"\\n Top 10 Pincodes with Unusually High Child Ratio:\")\n",
    "high_child = pincode_stats[pincode_stats['pct_0_5'] > 30].nlargest(10, 'pct_0_5')\n",
    "for _, row in high_child.iterrows():\n",
    "    print(f\"   {row['pincode']} ({row['state'][:20]}): {row['pct_0_5']:.1f}% age 0-5\")\n",
    "\n",
    "# Scatter plot of age distributions\n",
    "fig_age_dist = px.scatter(\n",
    "    pincode_stats.sample(min(5000, len(pincode_stats))),\n",
    "    x='pct_0_5',\n",
    "    y='pct_18_plus',\n",
    "    color='risk_score',\n",
    "    size='total_enrolments',\n",
    "    hover_data=['pincode', 'state', 'district'],\n",
    "    title='<b>AGE DISTRIBUTION PATTERNS</b><br><sup>Detecting Unusual Enrollment Demographics</sup>',\n",
    "    labels={'pct_0_5': '% Age 0-5', 'pct_18_plus': '% Age 18+', 'risk_score': 'Risk Score'},\n",
    "    color_continuous_scale='RdYlGn_r'\n",
    ")\n",
    "\n",
    "fig_age_dist.update_layout(template='plotly_white', height=500)\n",
    "fig_age_dist.write_html(f\"{OUTPUT_DIR}/charts/04_age_distribution.html\")\n",
    "print(\" Age distribution chart saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2893e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 9: Temporal Anomaly Patterns\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n⏰ TEMPORAL ANOMALY PATTERNS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Group by date to find anomalous days\n",
    "daily_stats = df_enrolment.groupby('date').agg({\n",
    "    'total_enrolments': 'sum',\n",
    "    'pincode': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "daily_stats.columns = ['date', 'daily_enrolments', 'active_pincodes']\n",
    "\n",
    "# Calculate rolling statistics\n",
    "daily_stats['rolling_mean'] = daily_stats['daily_enrolments'].rolling(window=7, min_periods=1).mean()\n",
    "daily_stats['rolling_std'] = daily_stats['daily_enrolments'].rolling(window=7, min_periods=1).std().fillna(0)\n",
    "\n",
    "# Flag anomalous days (outside 2 std)\n",
    "daily_stats['z_score'] = ((daily_stats['daily_enrolments'] - daily_stats['rolling_mean']) / \n",
    "                          daily_stats['rolling_std'].replace(0, 1))\n",
    "daily_stats['is_anomaly'] = abs(daily_stats['z_score']) > 2\n",
    "\n",
    "anomaly_days = daily_stats[daily_stats['is_anomaly']]\n",
    "\n",
    "print(f\"\\n TEMPORAL ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"   Total Days Analyzed: {len(daily_stats):,}\")\n",
    "print(f\"   Anomalous Days Detected: {len(anomaly_days):,}\")\n",
    "print(f\"   Average Daily Enrollments: {daily_stats['daily_enrolments'].mean():,.0f}\")\n",
    "\n",
    "if len(anomaly_days) > 0:\n",
    "    print(f\"\\n TOP ANOMALOUS DAYS:\")\n",
    "    for _, row in anomaly_days.nlargest(5, 'z_score').iterrows():\n",
    "        print(f\"   {row['date'].strftime('%Y-%m-%d')}: {row['daily_enrolments']:,.0f} (Z={row['z_score']:.2f})\")\n",
    "\n",
    "# Visualize\n",
    "fig_temporal = go.Figure()\n",
    "\n",
    "fig_temporal.add_trace(go.Scatter(\n",
    "    x=daily_stats['date'],\n",
    "    y=daily_stats['daily_enrolments'],\n",
    "    mode='lines',\n",
    "    name='Daily Enrollments',\n",
    "    line=dict(color='#3498db')\n",
    "))\n",
    "\n",
    "fig_temporal.add_trace(go.Scatter(\n",
    "    x=daily_stats['date'],\n",
    "    y=daily_stats['rolling_mean'],\n",
    "    mode='lines',\n",
    "    name='7-day Rolling Mean',\n",
    "    line=dict(color='#2ecc71', dash='dash')\n",
    "))\n",
    "\n",
    "# Mark anomalies\n",
    "if len(anomaly_days) > 0:\n",
    "    fig_temporal.add_trace(go.Scatter(\n",
    "        x=anomaly_days['date'],\n",
    "        y=anomaly_days['daily_enrolments'],\n",
    "        mode='markers',\n",
    "        name='Anomalies',\n",
    "        marker=dict(color='red', size=10, symbol='x')\n",
    "    ))\n",
    "\n",
    "fig_temporal.update_layout(\n",
    "    title='<b>DAILY ENROLLMENT PATTERNS & ANOMALIES</b>',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Enrollments',\n",
    "    template='plotly_white',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig_temporal.write_html(f\"{OUTPUT_DIR}/charts/04_temporal_anomalies.html\")\n",
    "print(\" Temporal anomaly chart saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd4487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 10: State-Level Anomaly Summary\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n STATE-LEVEL ANOMALY SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Group anomalies by state\n",
    "anomaly_by_state = anomalies.groupby('state').agg({\n",
    "    'pincode': 'count',\n",
    "    'total_enrolments': 'sum',\n",
    "    'risk_score': 'mean',\n",
    "    'cv_enrol': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "anomaly_by_state.columns = ['State', 'Anomalous Pincodes', 'Total Enrollments', 'Avg Risk', 'Avg CV%']\n",
    "anomaly_by_state = anomaly_by_state.sort_values('Anomalous Pincodes', ascending=False)\n",
    "\n",
    "print(\"\\n TOP STATES WITH ANOMALIES:\")\n",
    "print(\"-\" * 90)\n",
    "print(f\"{'State':<30} {'Anomalous Pincodes':>18} {'Total Enrollments':>18} {'Avg Risk':>12}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for _, row in anomaly_by_state.head(10).iterrows():\n",
    "    print(f\"{str(row['State'])[:29]:<30} {row['Anomalous Pincodes']:>18,} {row['Total Enrollments']:>18,.0f} {row['Avg Risk']:>12.1f}\")\n",
    "\n",
    "print(f\"\\n Summary:\")\n",
    "print(f\"   States with Anomalies: {len(anomaly_by_state):,}\")\n",
    "print(f\"   Total Anomalous Pincodes: {anomaly_by_state['Anomalous Pincodes'].sum():,}\")\n",
    "print(f\"   Total Suspicious Enrollments: {anomaly_by_state['Total Enrollments'].sum():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa8bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 11: Fraud Prevention Value Calculation\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n FRAUD PREVENTION VALUE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Parameters\n",
    "AVG_FRAUD_VALUE = 20000  # ₹20,000 average fraud amount per case\n",
    "DETECTION_RATE = 0.85    # 85% detection rate with system\n",
    "FALSE_POSITIVE_COST = 500  # ₹500 cost per false positive investigation\n",
    "\n",
    "# Calculate fraud metrics\n",
    "total_anomalous_enrolments = anomalies['total_enrolments'].sum()\n",
    "\n",
    "# Estimate fraud value (assume 5% of anomalous enrollments are fraudulent)\n",
    "fraud_rate = 0.05\n",
    "estimated_fraud_cases = total_anomalous_enrolments * fraud_rate\n",
    "potential_fraud_value = estimated_fraud_cases * AVG_FRAUD_VALUE\n",
    "preventable_fraud = potential_fraud_value * DETECTION_RATE\n",
    "\n",
    "# False positive cost (assume 20% false positive rate)\n",
    "false_positive_rate = 0.20\n",
    "false_positive_investigations = len(anomalies) * false_positive_rate\n",
    "false_positive_cost = false_positive_investigations * FALSE_POSITIVE_COST\n",
    "\n",
    "# Net savings\n",
    "net_savings = preventable_fraud - false_positive_cost\n",
    "\n",
    "print(\"\\n FRAUD PREVENTION METRICS:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"   Anomalous Pincodes: {len(anomalies):,}\")\n",
    "print(f\"   Suspicious Enrollments: {total_anomalous_enrolments:,.0f}\")\n",
    "print(f\"   Estimated Fraud Cases (5%): {estimated_fraud_cases:,.0f}\")\n",
    "print(f\"\\n   Potential Fraud Value: ₹{potential_fraud_value/10000000:.1f} Crore\")\n",
    "print(f\"   Preventable (85% detection): ₹{preventable_fraud/10000000:.1f} Crore\")\n",
    "print(f\"   False Positive Cost: ₹{false_positive_cost/100000:.2f} Lakhs\")\n",
    "print(f\"\\n    NET SAVINGS: ₹{net_savings/10000000:.2f} Crore\")\n",
    "\n",
    "# Annualize\n",
    "if len(daily_stats) > 0:\n",
    "    data_period_days = (daily_stats['date'].max() - daily_stats['date'].min()).days\n",
    "    if data_period_days > 0:\n",
    "        annual_factor = 365 / data_period_days\n",
    "        annual_savings = net_savings * annual_factor\n",
    "        print(f\"\\n    PROJECTED ANNUAL SAVINGS: ₹{annual_savings/10000000:.0f}-{annual_savings*1.2/10000000:.0f} Crore\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ce6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 12: Summary Dashboard\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n CREATING SUMMARY DASHBOARD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comprehensive anomaly dashboard\n",
    "fig_dashboard = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Risk Score Distribution',\n",
    "        'Anomalies by State (Top 10)',\n",
    "        'CV% vs Total Enrollments',\n",
    "        'Age Distribution vs Risk'\n",
    "    ),\n",
    "    specs=[[{\"type\": \"histogram\"}, {\"type\": \"bar\"}],\n",
    "           [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}]]\n",
    ")\n",
    "\n",
    "# Risk Score Distribution\n",
    "fig_dashboard.add_trace(\n",
    "    go.Histogram(x=pincode_stats['risk_score'], nbinsx=30, marker_color='#FF6B35'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Anomalies by State\n",
    "anomaly_state_counts = anomalies['state'].value_counts().head(10)\n",
    "fig_dashboard.add_trace(\n",
    "    go.Bar(x=anomaly_state_counts.values, y=anomaly_state_counts.index, \n",
    "           orientation='h', marker_color='#D62828'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# CV vs Total Enrollments\n",
    "sample_data = pincode_stats.sample(min(3000, len(pincode_stats)))\n",
    "fig_dashboard.add_trace(\n",
    "    go.Scatter(\n",
    "        x=sample_data['cv_enrol'],\n",
    "        y=sample_data['total_enrolments'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            color=sample_data['risk_score'],\n",
    "            colorscale='RdYlGn_r',\n",
    "            showscale=False\n",
    "        )\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Age distribution vs Risk\n",
    "fig_dashboard.add_trace(\n",
    "    go.Scatter(\n",
    "        x=sample_data['pct_0_5'],\n",
    "        y=sample_data['pct_18_plus'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            color=sample_data['risk_score'],\n",
    "            colorscale='RdYlGn_r',\n",
    "            showscale=True,\n",
    "            colorbar=dict(title='Risk')\n",
    "        )\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig_dashboard.update_layout(\n",
    "    title=dict(\n",
    "        text='<b>FRAUD DETECTION DASHBOARD</b><br><sup>Isolation Forest Anomaly Detection Results</sup>',\n",
    "        x=0.5\n",
    "    ),\n",
    "    height=700,\n",
    "    showlegend=False,\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig_dashboard.write_html(f\"{OUTPUT_DIR}/charts/04_anomaly_dashboard.html\")\n",
    "print(\" Dashboard saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5539d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 13: Save Results\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n SAVING ANOMALY DETECTION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save pincode risk scores\n",
    "pincode_stats.to_csv(f\"{OUTPUT_DIR}/04_pincode_risk_scores.csv\", index=False)\n",
    "print(\" Pincode risk scores saved\")\n",
    "\n",
    "# Save anomalies\n",
    "anomalies.to_csv(f\"{OUTPUT_DIR}/04_detected_anomalies.csv\", index=False)\n",
    "print(\" Detected anomalies saved\")\n",
    "\n",
    "# Save high risk pincodes\n",
    "high_risk.to_csv(f\"{OUTPUT_DIR}/04_high_risk_pincodes.csv\", index=False)\n",
    "print(\" High risk pincodes saved\")\n",
    "\n",
    "# Save state-level risk\n",
    "state_risk_agg.to_csv(f\"{OUTPUT_DIR}/04_state_risk_summary.csv\", index=False)\n",
    "print(\" State risk summary saved\")\n",
    "\n",
    "# Save anomaly by state\n",
    "anomaly_by_state.to_csv(f\"{OUTPUT_DIR}/04_anomalies_by_state.csv\", index=False)\n",
    "print(\" Anomalies by state saved\")\n",
    "\n",
    "# Save daily anomalies\n",
    "daily_stats.to_csv(f\"{OUTPUT_DIR}/04_daily_stats.csv\", index=False)\n",
    "print(\" Daily statistics saved\")\n",
    "\n",
    "print(f\"\\n All outputs saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e56c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 14: Key Insights Summary\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" ANOMALY DETECTION - KEY INSIGHTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    " DETECTION SUMMARY:\n",
    "   • Total Pincodes Analyzed: {len(pincode_stats):,}\n",
    "   • Anomalies Detected: {len(anomalies):,} ({len(anomalies)/len(pincode_stats)*100:.1f}%)\n",
    "   • High-Risk Pincodes (Score>70): {len(high_risk):,}\n",
    "   • Fraud Clusters Identified: {n_clusters}\n",
    "\n",
    " GEOGRAPHIC INSIGHTS:\n",
    "   • States with Anomalies: {len(anomaly_by_state):,}\n",
    "   • Top Anomaly State: {anomaly_by_state.iloc[0]['State'] if len(anomaly_by_state) > 0 else 'N/A'}\n",
    "\n",
    " TEMPORAL INSIGHTS:\n",
    "   • Anomalous Days: {len(anomaly_days):,}\n",
    "   • Data Period: {daily_stats['date'].min().strftime('%Y-%m-%d')} to {daily_stats['date'].max().strftime('%Y-%m-%d')}\n",
    "\n",
    " FRAUD PREVENTION:\n",
    "   • Suspicious Enrollments: {total_anomalous_enrolments:,.0f}\n",
    "   • Estimated Fraud Value: ₹{potential_fraud_value/10000000:.1f} Crore\n",
    "   • Preventable Fraud: ₹{preventable_fraud/10000000:.1f} Crore\n",
    "\n",
    " OUTPUTS GENERATED:\n",
    "   • 6 CSV files with analysis results\n",
    "   • 5 Interactive HTML charts\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" NOTEBOOK 04 COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}